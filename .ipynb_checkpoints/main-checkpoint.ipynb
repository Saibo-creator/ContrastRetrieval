{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:08:30.422565Z",
     "start_time": "2020-12-03T12:08:29.328196Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_loaders.AUS_dataset import AUSDataset, AUSPytorchDataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from project_settings import EOS_TOK, EOC_TOK\n",
    "import pdb\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "from models.Model import MeanModel, TruncatModel, NNModel\n",
    "from project_settings import ExpConfig, DatasetConfig\n",
    "from utils import chunkify, encode_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T10:14:48.872840Z",
     "start_time": "2020-12-03T10:14:48.868101Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def micro_contrast(left_left, left_right, right_right, right_left):\n",
    "    count = (left_left.numel() + left_right.numel() + right_right.numel() + right_left.numel()) / 2\n",
    "    sum = torch.sum(left_left) + torch.sum(right_right) - torch.sum(left_right) - torch.sum(right_left)\n",
    "    return sum / count\n",
    "\n",
    "\n",
    "# def micro_contrast(*arg):\n",
    "#     count=0\n",
    "#     sum=0\n",
    "#     for tensor in arg:\n",
    "#         count+=tensor.numel()\n",
    "#         sum+=torch.sum(tensor)\n",
    "#     return sum/count\n",
    "\n",
    "def macro_contrast(left_left, left_right, right_right, right_left):\n",
    "    return (torch.mean(left_left) - torch.mean(left_right) + torch.mean(right_right) - torch.mean(right_left)) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:54:08.441412Z",
     "start_time": "2020-12-02T13:54:08.437871Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_chunk_to_dict(chunk):\n",
    "    n,dim=chunk.size()\n",
    "    chunk_dict={}\n",
    "    chunk_dict[\"input_ids\"]=chunk[0]\n",
    "    chunk_dict[\"token_type_ids\"]=chunk[1]\n",
    "    chunk_dict[\"attention_mask\"]=chunk[2]\n",
    "    return chunk_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the transformer via a **retrieval task**. We want to pair case **description** with the right **catchphrases** for each case in our legal dataset.\n",
    "\n",
    "As seen preivously in the **dataset analysis**, the case description are in general very long, average length is **34k** chars,thus around **6k tokens**. However, transformer relying to squared attention only takes 512 tokens. So in this very naive baseline benchmark, we just truncate the sentence at **512th token** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T10:45:00.922217Z",
     "start_time": "2020-12-03T10:44:57.787985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_a length: 11888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'macro_contrast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eca11ef278f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mnb_test_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtest_macro_contrast\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmacro_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mtest_micro_contrast\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmicro_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'macro_contrast' is not defined"
     ]
    }
   ],
   "source": [
    "# cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "ds = AUSDataset()\n",
    "exp_config = ExpConfig(\"MeanModel\")\n",
    "train_dataloader = ds.get_data_loader(split='train', batch_size=2, shuffle=True)\n",
    "val_dataloader = ds.get_data_loader(split='val', batch_size=2, shuffle=True)\n",
    "test_dataloader = ds.get_data_loader(split='test', batch_size=2, shuffle=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(exp_config.uri)\n",
    "encoder = AutoModel.from_pretrained(exp_config.uri) \n",
    "test_micro_contrast = 0  # running loss\n",
    "test_macro_contrast = 0\n",
    "nb_test_steps = 0\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(test_dataloader)):\n",
    "    # Add batch to GPU\n",
    "    \"batch = tuple(t.to(device) for t in batch)\"\n",
    "    # Unpack the inputs from our dataloader\n",
    "    sentences, catchphrases = batch  # len(sentences)=2, len(catchphrases)=2\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "\n",
    "    sentences_a, catchphrase_a = sentences[0], catchphrases[0]\n",
    "    sentences_b, catchphrase_b = sentences[1], catchphrases[1]\n",
    "    batch_catchphrase_a = catchphrase_a.split(EOC_TOK)\n",
    "    batch_catchphrase_b = catchphrase_b.split(EOC_TOK)\n",
    "\n",
    "    encoded_batch_catchphrase_a = tokenizer(batch_catchphrase_a, truncation=True, return_tensors=\"pt\",\n",
    "                                            padding='max_length', max_length=512)\n",
    "    encoded_batch_catchphrase_b = tokenizer(batch_catchphrase_b, truncation=True, return_tensors=\"pt\",\n",
    "                                            padding='max_length', max_length=512)\n",
    "\n",
    "    encoded_sentence_a = tokenizer(sentences_a, truncation=True, return_tensors=\"pt\", padding='max_length',\n",
    "                                   max_length=512)\n",
    "    encoded_sentence_b = tokenizer(sentences_b, truncation=True, return_tensors=\"pt\", padding='max_length',\n",
    "                                   max_length=512)\n",
    "    print(\"sentences_a length:\", len(sentences_a))\n",
    "    _, batch_catchphrase_embedding_a = encoder(**encoded_batch_catchphrase_a)  # [7, 768]\n",
    "    _, batch_catchphrase_embedding_b = encoder(**encoded_batch_catchphrase_b)  # [13,768]\n",
    "\n",
    "    _, sentence_embedding_a = encoder(**encoded_sentence_a)  # [1, 768]\n",
    "    _, sentence_embedding_b = encoder(**encoded_sentence_b)  # [1, 768]\n",
    "\n",
    "    left_left = torch.cdist(sentence_embedding_a, batch_catchphrase_embedding_a, p=2.0)  # [1, 768]*[7, 768]=[1, 7]\n",
    "    left_right = torch.cdist(sentence_embedding_a, batch_catchphrase_embedding_b,\n",
    "                             p=2.0)  # [1, 768]*[13, 768]=[1, 13]\n",
    "\n",
    "    right_right = torch.cdist(sentence_embedding_b, batch_catchphrase_embedding_b,\n",
    "                              p=2.0)  # [1, 768]*[13, 768]=[1, 13]\n",
    "    right_left = torch.cdist(sentence_embedding_b, batch_catchphrase_embedding_a, p=2.0)  # [1, 768]*[7, 768]=[1, 7]\n",
    "\n",
    "    nb_test_steps += 1\n",
    "    test_macro_contrast += macro_contrast(left_left, left_right, right_right, right_left)\n",
    "    test_micro_contrast += micro_contrast(left_left, left_right, right_right, right_left)\n",
    "\n",
    "    print(\"Test micro contrast: {}\".format(test_micro_contrast / nb_test_steps))\n",
    "    print(\"Test macro contrast: {}\".format(test_macro_contrast / nb_test_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:08:43.304726Z",
     "start_time": "2020-12-03T12:08:35.273280Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x12659a850>\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 7/7 [00:00<00:00, 565.79it/s]\n",
      "\n",
      "1it [00:02,  2.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0587562322616577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 674.11it/s]\n",
      "\n",
      "2it [00:04,  2.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.423868715763092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.25s/it]\n",
      "Epoch:   0%|          | 0/10 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3baedfab37b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m#         chunk_embeddings_a=torch.stack(chunk_embeddings_a,dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mchunk_embeddings_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_indices_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mchunk_embeddings_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_indices_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AEPFL_S3/project/ContrastRetrieval/utils.py\u001b[0m in \u001b[0;36mencode_chunks\u001b[0;34m(chunk_indices, encoder)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mchunk_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_indice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mchunk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [1, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mchunk_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mchunk_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m         )\n\u001b[1;32m    850\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m                 )\n\u001b[1;32m    485\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         )\n\u001b[1;32m    341\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/contrastRetrieval/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# def train_contrast_retrieval(data_config, exp_config):\n",
    "\n",
    "\n",
    "# cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "ds = AUSDataset()\n",
    "train_dataloader = ds.get_data_loader(split='train', batch_size=2, shuffle=True)\n",
    "val_dataloader = ds.get_data_loader(split='val', batch_size=2, shuffle=True)\n",
    "test_dataloader = ds.get_data_loader(split='test', batch_size=2, shuffle=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(exp_config.uri)\n",
    "encoder = AutoModel.from_pretrained(exp_config.uri)\n",
    "\n",
    "model = NNModel(exp_config)\n",
    "\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "print(encoder.parameters())\n",
    "\n",
    "for name, param in encoder.named_parameters():\n",
    "    print(name, param.size())\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = exp_config.epochs\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch__ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    print(\"start training\")\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    encoder.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0  # running loss\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        # Unpack the inputs from our dataloader\n",
    "        sentences, catchphrases = batch  # len(sentences)=2, len(catchphrases)=2\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "\n",
    "        sentences_a, catchphrase_a = sentences[0], catchphrases[0]\n",
    "        sentences_b, catchphrase_b = sentences[1], catchphrases[1]\n",
    "\n",
    "        batch_catchphrase_a = catchphrase_a.split(EOC_TOK)\n",
    "        batch_catchphrase_b = catchphrase_b.split(EOC_TOK)\n",
    "\n",
    "        encoded_batch_catchphrase_a = tokenizer(batch_catchphrase_a, truncation=True, return_tensors=\"pt\",\n",
    "                                                padding='max_length', max_length=128)\n",
    "        encoded_batch_catchphrase_b = tokenizer(batch_catchphrase_b, truncation=True, return_tensors=\"pt\",\n",
    "                                                padding='max_length', max_length=128)\n",
    "\n",
    "        sentence_indices_a = tokenizer(sentences_a, truncation=True, return_tensors=\"pt\", padding='max_length',\n",
    "                                       max_length=512*12)\n",
    "        sentence_indices_b = tokenizer(sentences_b, truncation=True, return_tensors=\"pt\", padding='max_length',\n",
    "                                       max_length=512*12)\n",
    "\n",
    "        _, batch_catchphrase_embedding_a = encoder(**encoded_batch_catchphrase_a)  # [7, 768]\n",
    "        _, batch_catchphrase_embedding_b = encoder(**encoded_batch_catchphrase_b)  # [13,768]\n",
    "\n",
    "        \n",
    "        chunk_indices_a=chunkify(sentence_indices_a)\n",
    "        \n",
    "        chunk_indices_b=chunkify(sentence_indices_b)\n",
    "        \n",
    "#         chunk_embeddings_a=[]\n",
    "#         chunk_embeddings_b=[]\n",
    "#         for i, chunk_indice in enumerate(chunk_indices_a):\n",
    "#             _, chunk_embedding= encoder(**chunk_indices_a[i])  # [1, 768]\n",
    "#             chunk_embeddings_a.append(torch.squeeze(chunk_embedding))\n",
    "        \n",
    "#         chunk_embeddings_a=torch.stack(chunk_embeddings_a,dim=0)\n",
    "\n",
    "        chunk_embeddings_a=encode_chunks(chunk_indices_a,encoder)\n",
    "        chunk_embeddings_b=encode_chunks(chunk_indices_b,encoder)\n",
    "    \n",
    "        \n",
    "        #################### Aggregation ######################\n",
    "        sentence_embedding_a=torch.mean(chunk_embeddings_a,dim=0)\n",
    "        sentence_embedding_b=torch.mean(chunk_embeddings_b,dim=0)\n",
    "        \n",
    "\n",
    "        triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "        anchor_a = sentence_embedding_a.unsqueeze(0)\n",
    "        anchor_b = sentence_embedding_b.unsqueeze(0)\n",
    "        batch_train_loss_set=[]\n",
    "        for catchphrase_embedding_a in tqdm(batch_catchphrase_embedding_a):\n",
    "            for catchphrase_embedding_b in batch_catchphrase_embedding_b:\n",
    "                positive = catchphrase_embedding_a.unsqueeze(0)\n",
    "                negative = catchphrase_embedding_b.unsqueeze(0)\n",
    "                loss = triplet_loss(anchor_a, positive, negative)\n",
    "                batch_train_loss_set.append(loss.unsqueeze(0))\n",
    "                loss = triplet_loss(anchor_b, negative, positive)\n",
    "                batch_train_loss_set.append(loss.unsqueeze(0))\n",
    "        batch_loss=torch.mean(torch.cat(batch_train_loss_set))\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += batch_loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T12:08:33.680801Z",
     "start_time": "2020-12-03T12:08:33.543607Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_contrast_retrieval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-93c22f244bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mexp_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MeanModel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_contrast_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_contrast_retrieval' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ds = AUSDataset()\n",
    "\n",
    "    # ds.save_processed_splits()\n",
    "\n",
    "    test_dataloader = ds.get_data_loader(split='test', batch_size=2, shuffle=True)\n",
    "    # print(test_dl.batch_size)\n",
    "    # for i in test_dl:\n",
    "    #     print(len(i[0]),len(i[1]))\n",
    "    data_config = DatasetConfig(\"AUS\")\n",
    "    exp_config = ExpConfig(\"MeanModel\")\n",
    "    train_contrast_retrieval(data_config, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
